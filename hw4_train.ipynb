{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install torch\n",
    "# !pip install sklearn\n",
    "# !pip install pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import os.path\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure reproducibility\n",
    "Use a fixed seed such that all steps and results can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 544\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Credit: From PyTorch's documentation\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fname, test_dataset=False):\n",
    "    sentences = []\n",
    "    with open(fname, 'r') as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "        sentence_words = []\n",
    "        sentence_tags = []\n",
    "        for line in lines:\n",
    "            if line:\n",
    "                # test data has only index and word\n",
    "                if test_dataset:\n",
    "                    index, word = line.split()\n",
    "                    sentence_words.append(word)\n",
    "                # train/dev data has index, word, and tag\n",
    "                else:\n",
    "                    index, word, tag = line.split()\n",
    "                    sentence_words.append(word)\n",
    "                    sentence_tags.append(tag)\n",
    "            else:\n",
    "                # Create a sentence upon reaching an empty new line\n",
    "                if test_dataset:\n",
    "                    sentences.append(sentence_words)\n",
    "                else:\n",
    "                    sentences.append((sentence_words, sentence_tags))\n",
    "                sentence_words = []\n",
    "                sentence_tags = []\n",
    "        # Create a sentence for the last sentence in the document\n",
    "        # incase it missed a newline in the document at the end\n",
    "        if len(sentence_words) > 0:\n",
    "            if test_dataset:\n",
    "                sentences.append(sentence_words)\n",
    "            else:\n",
    "                sentences.append((sentence_words, sentence_tags))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all datasets given\n",
    "train_data = read_data('data/train')\n",
    "dev_data = read_data('data/dev')\n",
    "test_data = read_data('data/test', test_dataset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions to process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix, use_unk=False):\n",
    "    if use_unk:\n",
    "        indices = [to_ix[w] if w in to_ix else to_ix['<UNK>'] for w in seq]\n",
    "    else:\n",
    "        indices = [to_ix[w] for w in seq]\n",
    "    return indices\n",
    "\n",
    "def get_spelling_feature(sentence):\n",
    "    result = []\n",
    "    for word in sentence:\n",
    "        # PAD = 0\n",
    "        if word == '<PAD>':\n",
    "            result.append(0)\n",
    "        ## ALL LOWER = 1\n",
    "        elif word.islower():\n",
    "            result.append(1)\n",
    "        # ALL UPPER = 2\n",
    "        elif word.isupper():\n",
    "            result.append(2)\n",
    "        # FIRST UPPER = 3\n",
    "        elif word[0].isupper():\n",
    "            result.append(3)\n",
    "        # OTHERS = 4\n",
    "        else:\n",
    "            result.append(4)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # Retrieves longest sentence, for padding\n",
    "        max_sentence_len = max([len(sentence) for sentence, tags in data])\n",
    "        self.X = []\n",
    "        self.X_original = []\n",
    "        self.y = []\n",
    "        self.X_spelling = []\n",
    "        \n",
    "        for sentence, tags in data:\n",
    "            # Pad the sentences to the same length\n",
    "            padded_sentence = sentence.copy()\n",
    "            padded_tags = tags.copy()\n",
    "            while len(padded_sentence) < max_sentence_len:\n",
    "                padded_sentence.append('<PAD>')\n",
    "                padded_tags.append('<PAD>')\n",
    "            # Convert to indices\n",
    "            transformed_sentence = prepare_sequence(padded_sentence, word_to_ix, use_unk=True)\n",
    "            transformed_tags = prepare_sequence(padded_tags, tag_to_ix)\n",
    "            # Get spelling indices\n",
    "            spelling_sentence = get_spelling_feature(padded_sentence)\n",
    "            # Add to dataset\n",
    "            self.X.append(transformed_sentence)\n",
    "            self.X_original.append(padded_sentence)\n",
    "            self.y.append(transformed_tags)\n",
    "            self.X_spelling.append(spelling_sentence)\n",
    "            \n",
    "        self.X = torch.from_numpy(np.array(self.X, dtype=np.int64)).to(device)\n",
    "        self.y = torch.from_numpy(np.array(self.y, dtype=np.int64)).to(device)\n",
    "        self.X_spelling = torch.from_numpy(np.array(self.X_spelling, dtype=np.int64)).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index], self.X_original[index], self.X_spelling[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Simple Bidirectional LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "VOCAB_THRESHOLD = 3\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "LSTM_HIDDEN_DIM = 256\n",
    "LSTM_DROPOUT = 0.33\n",
    "LINEAR_DIM = 128\n",
    "\n",
    "LEARNING_RATE = 0.2\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "ELU_ALPHA = 0.5\n",
    "\n",
    "SCHEDULER_STEP_SIZE = 5\n",
    "SCHEDULER_GAMMA = 0.5\n",
    "\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate vocab and word/tag -> index, and index -> word/tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate vocab\n",
    "words_freq = defaultdict(int)\n",
    "for sentence, tags in train_data:\n",
    "    for word in sentence:\n",
    "        words_freq[word] += 1\n",
    "        \n",
    "vocab = {key for key, val in words_freq.items() if val >= VOCAB_THRESHOLD}\n",
    "\n",
    "# Generate word/tag to index mappings\n",
    "word_to_ix = {'<PAD>': 0, '<UNK>': 1}\n",
    "tag_to_ix = {'<PAD>': 0}\n",
    "for sentence, tags in train_data:\n",
    "    for word in sentence:\n",
    "        if word not in vocab:\n",
    "            word = '<UNK>'\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "            \n",
    "# Generate index to word/tag mappings\n",
    "ix_to_word = {v: k for k, v in word_to_ix.items()}\n",
    "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
    "\n",
    "# Calculate the size of vocabulary & tags\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "TAGS_SIZE = len(tag_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM Model with random embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM1(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, linear_dim, tags_size, lstm_dropout, elu_alpha):\n",
    "        super(BLSTM1, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=word_to_ix['<PAD>'])\n",
    "        self.dropout_pre_lstm = nn.Dropout(lstm_dropout)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout_post_lstm = nn.Dropout(lstm_dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, linear_dim)\n",
    "        self.elu = nn.ELU(alpha=elu_alpha)\n",
    "        self.linear2 = nn.Linear(linear_dim, tags_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout_pre_lstm(x)\n",
    "        \n",
    "        h0 = torch.zeros(2, x.size(0), self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(2, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        out = self.dropout_post_lstm(out)\n",
    "        out = self.linear(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.linear2(out)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to predict on a development data loader\n",
    "# Writes the output to a file, i.e. to dev.out\n",
    "def predict_dev1(model, data_loader, fname):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y, X_original, X_spelling in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred_scores = model(X)\n",
    "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
    "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
    "\n",
    "            idx = 1\n",
    "            output = []\n",
    "            for i in range(len(y_pred_flat)):\n",
    "                word = X_original[i][0]\n",
    "                pred = ix_to_tag[y_pred_flat[i]]\n",
    "                if word == '<PAD>':\n",
    "                    break\n",
    "                output.append((idx, word, pred))\n",
    "                idx += 1\n",
    "            outputs.append(output)\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        for i in range(len(outputs)):\n",
    "            for j in range(len(outputs[i])):\n",
    "                idx, word, pred = outputs[i][j]\n",
    "                f.write(f'{idx} {word} {pred}\\n')\n",
    "            if i != len(outputs)-1:\n",
    "                f.write('\\n')\n",
    "\n",
    "# Used to predict on a test data, list of sentences\n",
    "# Writes the output to a file, i.e. to test.out\n",
    "def predict_test1(model, sentences, fname):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sentence in sentences:\n",
    "            spelling_sentence = [get_spelling_feature(sentence)]\n",
    "            spelling_sentence = torch.from_numpy(np.array(spelling_sentence, dtype=np.int64)).to(device)\n",
    "            \n",
    "            transformed_sentence = [prepare_sequence(sentence, word_to_ix, use_unk=True)]\n",
    "            transformed_sentence = torch.from_numpy(np.array(transformed_sentence, dtype=np.int64)).to(device)\n",
    "            \n",
    "            y_pred_scores = model(transformed_sentence)\n",
    "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
    "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
    "\n",
    "            idx = 1\n",
    "            output = []\n",
    "            for i in range(len(y_pred_flat)):\n",
    "                word = sentence[i]\n",
    "                pred = ix_to_tag[y_pred_flat[i]]\n",
    "                if word == '<PAD>':\n",
    "                    break\n",
    "                output.append((idx, word, pred))\n",
    "                idx += 1\n",
    "            outputs.append(output)\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        for i in range(len(outputs)):\n",
    "            for j in range(len(outputs[i])):\n",
    "                idx, word, pred = outputs[i][j]\n",
    "                f.write(f'{idx} {word} {pred}\\n')\n",
    "            if i != len(outputs)-1:\n",
    "                f.write('\\n')\n",
    "                \n",
    "# Used to predict on a development data loader\n",
    "# Writes statistics to console\n",
    "def predict1(model, data_loader, message):\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y, X_original, X_spelling in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred_scores = model(X)\n",
    "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
    "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
    "            y_flat = torch.flatten(y).tolist()\n",
    "            \n",
    "            for i in range(len(y_pred_flat)):\n",
    "                if y_flat[i] == tag_to_ix['<PAD>']:\n",
    "                    break\n",
    "                all_y.append(y_flat[i])\n",
    "                all_y_pred.append(y_pred_flat[i])\n",
    "\n",
    "    print(message, classification_report(all_y, all_y_pred))\n",
    "                \n",
    "# Used to predict on a development data loader\n",
    "# Writes the output to a file for PERL script, i.e. to prediction.txt\n",
    "def predict_perl1(model, data_loader, fname):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y, X_original, X_spelling in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred_scores = model(X)\n",
    "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
    "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
    "            y_flat = torch.flatten(y).tolist()\n",
    "\n",
    "            idx = 1\n",
    "            output = []\n",
    "            for i in range(len(y_pred_flat)):\n",
    "                word = X_original[i][0]\n",
    "                gold = ix_to_tag[y_flat[i]]\n",
    "                pred = ix_to_tag[y_pred_flat[i]]\n",
    "                if word == '<PAD>':\n",
    "                    break\n",
    "                output.append((idx, word, gold, pred))\n",
    "                idx += 1\n",
    "            outputs.append(output)\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        for i in range(len(outputs)):\n",
    "            for j in range(len(outputs[i])):\n",
    "                idx, word, gold, pred = outputs[i][j]\n",
    "                f.write(f'{idx} {word} {gold} {pred}\\n')\n",
    "            if i != len(outputs)-1:\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dev_dataset = NERDataset(dev_data)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BLSTM1(VOCAB_SIZE, EMBEDDING_DIM, LSTM_HIDDEN_DIM, LINEAR_DIM, TAGS_SIZE, LSTM_DROPOUT, ELU_ALPHA).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=SCHEDULER_STEP_SIZE, gamma=SCHEDULER_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLSTM1(\n",
       "  (embeddings): Embedding(8129, 100, padding_idx=0)\n",
       "  (dropout_pre_lstm): Dropout(p=0.33, inplace=False)\n",
       "  (lstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
       "  (dropout_post_lstm): Dropout(p=0.33, inplace=False)\n",
       "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (elu): ELU(alpha=0.5)\n",
       "  (linear2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 blstm1.pt does not exist. Training a new model...\n",
      "Epoch 1 / 20, training loss: 0.06235, learning rate: 0.20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanavesa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\sanavesa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\sanavesa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 20               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.85      0.23      0.36      1341\n",
      "           2       0.91      1.00      0.95     42975\n",
      "           3       0.85      0.43      0.57       922\n",
      "           4       0.88      0.41      0.56      1842\n",
      "           5       0.82      0.46      0.59      1307\n",
      "           6       0.90      0.52      0.66      1837\n",
      "           7       0.52      0.34      0.41       751\n",
      "           8       0.85      0.37      0.52       346\n",
      "           9       0.70      0.53      0.60       257\n",
      "\n",
      "    accuracy                           0.90     51578\n",
      "   macro avg       0.73      0.43      0.52     51578\n",
      "weighted avg       0.89      0.90      0.88     51578\n",
      "\n",
      "Epoch 2 / 20, training loss: 0.03990, learning rate: 0.20000\n",
      "Epoch 3 / 20, training loss: 0.03286, learning rate: 0.20000\n",
      "Epoch 4 / 20, training loss: 0.02879, learning rate: 0.20000\n",
      "Epoch 5 / 20, training loss: 0.02618, learning rate: 0.20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanavesa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\sanavesa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\sanavesa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 / 20               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.86      0.63      0.73      1341\n",
      "           2       0.96      0.99      0.98     42975\n",
      "           3       0.91      0.69      0.79       922\n",
      "           4       0.88      0.74      0.80      1842\n",
      "           5       0.74      0.91      0.82      1307\n",
      "           6       0.92      0.74      0.82      1837\n",
      "           7       0.86      0.70      0.77       751\n",
      "           8       0.85      0.59      0.69       346\n",
      "           9       0.85      0.72      0.78       257\n",
      "\n",
      "    accuracy                           0.95     51578\n",
      "   macro avg       0.78      0.67      0.72     51578\n",
      "weighted avg       0.95      0.95      0.94     51578\n",
      "\n",
      "Epoch 6 / 20, training loss: 0.02153, learning rate: 0.10000\n",
      "Epoch 7 / 20, training loss: 0.01961, learning rate: 0.10000\n",
      "Epoch 8 / 20, training loss: 0.01846, learning rate: 0.10000\n",
      "Epoch 9 / 20, training loss: 0.01747, learning rate: 0.10000\n",
      "Epoch 10 / 20, training loss: 0.01684, learning rate: 0.10000\n",
      "Epoch 10 / 20               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.70      0.75      1341\n",
      "           2       0.97      0.99      0.98     42975\n",
      "           3       0.91      0.75      0.82       922\n",
      "           4       0.90      0.78      0.84      1842\n",
      "           5       0.89      0.87      0.88      1307\n",
      "           6       0.92      0.81      0.86      1837\n",
      "           7       0.86      0.74      0.79       751\n",
      "           8       0.83      0.65      0.73       346\n",
      "           9       0.82      0.81      0.81       257\n",
      "\n",
      "    accuracy                           0.96     51578\n",
      "   macro avg       0.88      0.79      0.83     51578\n",
      "weighted avg       0.95      0.96      0.96     51578\n",
      "\n",
      "Epoch 11 / 20, training loss: 0.01517, learning rate: 0.05000\n",
      "Epoch 12 / 20, training loss: 0.01478, learning rate: 0.05000\n",
      "Epoch 13 / 20, training loss: 0.01405, learning rate: 0.05000\n",
      "Epoch 14 / 20, training loss: 0.01359, learning rate: 0.05000\n",
      "Epoch 15 / 20, training loss: 0.01343, learning rate: 0.05000\n",
      "Epoch 15 / 20               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.71      0.77      1341\n",
      "           2       0.97      0.99      0.98     42975\n",
      "           3       0.91      0.75      0.83       922\n",
      "           4       0.93      0.79      0.85      1842\n",
      "           5       0.94      0.82      0.88      1307\n",
      "           6       0.93      0.84      0.88      1837\n",
      "           7       0.84      0.77      0.80       751\n",
      "           8       0.83      0.68      0.75       346\n",
      "           9       0.89      0.79      0.84       257\n",
      "\n",
      "    accuracy                           0.96     51578\n",
      "   macro avg       0.90      0.79      0.84     51578\n",
      "weighted avg       0.96      0.96      0.96     51578\n",
      "\n",
      "Epoch 16 / 20, training loss: 0.01273, learning rate: 0.02500\n",
      "Epoch 17 / 20, training loss: 0.01230, learning rate: 0.02500\n",
      "Epoch 18 / 20, training loss: 0.01223, learning rate: 0.02500\n",
      "Epoch 19 / 20, training loss: 0.01169, learning rate: 0.02500\n",
      "Epoch 20 / 20, training loss: 0.01175, learning rate: 0.02500\n",
      "Epoch 20 / 20               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.72      0.79      1341\n",
      "           2       0.97      0.99      0.98     42975\n",
      "           3       0.93      0.75      0.83       922\n",
      "           4       0.92      0.80      0.86      1842\n",
      "           5       0.91      0.88      0.90      1307\n",
      "           6       0.92      0.85      0.88      1837\n",
      "           7       0.83      0.77      0.80       751\n",
      "           8       0.80      0.69      0.74       346\n",
      "           9       0.79      0.85      0.82       257\n",
      "\n",
      "    accuracy                           0.96     51578\n",
      "   macro avg       0.88      0.81      0.84     51578\n",
      "weighted avg       0.96      0.96      0.96     51578\n",
      "\n",
      "Wall time: 27min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if os.path.isfile('blstm1.pt'):\n",
    "    print('Task 1', 'blstm1.pt exists. Loading existing model...')\n",
    "    model = torch.load('blstm1.pt')\n",
    "    model.to(device)\n",
    "else:\n",
    "    print('Task 1', 'blstm1.pt does not exist. Training a new model...')\n",
    "    total_loss = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        for i, (X, y, X_original, X_spelling) in enumerate(train_loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred_scores = model(X)\n",
    "            y_pred = torch.flatten(y_pred_scores, start_dim=0, end_dim=1)\n",
    "            y = torch.flatten(y)\n",
    "            \n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss.append(loss.item())\n",
    "        print(f'Epoch {epoch+1} / {NUM_EPOCHS}, training loss: {np.average(total_loss):.5f}, learning rate: {optimizer.param_groups[0][\"lr\"]:.5f}')\n",
    "        total_loss = []\n",
    "        scheduler.step()\n",
    "        if epoch == 0 or (epoch+1) % 5 == 0:\n",
    "            predict1(model, dev_loader, f'Epoch {epoch+1} / {NUM_EPOCHS}')\n",
    "    torch.save(model, 'blstm1.pt')\n",
    "\n",
    "# Prediction for all cases (dev, test, and dev for perl)\n",
    "predict_perl1(model, dev_loader, 'prediction1.txt')\n",
    "predict_dev1(model, dev_loader, 'dev1.out')\n",
    "predict_test1(model, test_data, 'test1.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting structs for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_to_ix_1.pkl', 'wb') as f:\n",
    "    pickle.dump(word_to_ix, f)\n",
    "    \n",
    "with open('tag_to_ix_1.pkl', 'wb') as f:\n",
    "    pickle.dump(tag_to_ix, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Using GloVe word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "LSTM_HIDDEN_DIM = 256\n",
    "LSTM_DROPOUT = 0.33\n",
    "LINEAR_DIM = 128\n",
    "\n",
    "LEARNING_RATE = 0.3\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "ELU_ALPHA = 0.5\n",
    "\n",
    "SCHEDULER_STEP_SIZE = 5\n",
    "SCHEDULER_GAMMA = 0.5\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "SPELLING_EMBEDDING_DIM = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate vocab and word/tag -> index, and index -> word/tag, also load GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "vocab = set(['<PAD>', '<UNK>'])\n",
    "\n",
    "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector\n",
    "\n",
    "for sentence, tags in train_data:\n",
    "    vocab.update(sentence)\n",
    "for sentence, tags in dev_data:\n",
    "    vocab.update(sentence)\n",
    "for sentence in test_data:\n",
    "    vocab.update(sentence)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {v: k for k, v in word_to_ix.items()}\n",
    "\n",
    "embedding_matrix = np.zeros((len(vocab), EMBEDDING_DIM))\n",
    "for word in vocab:\n",
    "    index = word_to_ix[word]\n",
    "    if word in embeddings_dict:\n",
    "        vector = embeddings_dict[word]\n",
    "    elif word.lower() in embeddings_dict:\n",
    "        vector = embeddings_dict[word.lower()]\n",
    "    else:\n",
    "        vector = np.random.rand(EMBEDDING_DIM)\n",
    "    embedding_matrix[index] = vector\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM Model with GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, linear_dim, tags_size, lstm_dropout, elu_alpha, embeddings, spelling_embedding_dim):\n",
    "        super(BLSTM2, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embeddings_word = nn.Embedding.from_pretrained(torch.from_numpy(embeddings).float(), freeze=False, padding_idx=word_to_ix['<PAD>'])\n",
    "        self.embeddings_spelling = nn.Embedding(num_embeddings=5, embedding_dim=spelling_embedding_dim, padding_idx=0)\n",
    "        self.dropout_pre_lstm = nn.Dropout(lstm_dropout)\n",
    "        self.lstm = nn.LSTM(embedding_dim+spelling_embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout_post_lstm = nn.Dropout(lstm_dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, linear_dim)\n",
    "        self.elu = nn.ELU(alpha=elu_alpha)\n",
    "        self.linear2 = nn.Linear(linear_dim, tags_size)\n",
    "    \n",
    "    def forward(self, x_word, x_spelling):\n",
    "        x1 = self.embeddings_word(x_word)\n",
    "        x2 = self.embeddings_spelling(x_spelling)\n",
    "        x = torch.cat((x1, x2), dim=2).to(device)\n",
    "        x = self.dropout_pre_lstm(x)\n",
    "        \n",
    "        h0 = torch.zeros(2, x.size(0), self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(2, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        out = self.dropout_post_lstm(out)\n",
    "        out = self.linear(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.linear2(out)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to predict on a development data loader\n",
    "# Writes the output to a file, i.e. to dev.out\n",
    "def predict_dev2(model, data_loader, fname):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y, X_original, X_spelling in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred_scores = model(X, X_spelling)\n",
    "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
    "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
    "\n",
    "            idx = 1\n",
    "            output = []\n",
    "            for i in range(len(y_pred_flat)):\n",
    "                word = X_original[i][0]\n",
    "                pred = ix_to_tag[y_pred_flat[i]]\n",
    "                if word == '<PAD>':\n",
    "                    break\n",
    "                output.append((idx, word, pred))\n",
    "                idx += 1\n",
    "            outputs.append(output)\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        for i in range(len(outputs)):\n",
    "            for j in range(len(outputs[i])):\n",
    "                idx, word, pred = outputs[i][j]\n",
    "                f.write(f'{idx} {word} {pred}\\n')\n",
    "            if i != len(outputs)-1:\n",
    "                f.write('\\n')\n",
    "\n",
    "# Used to predict on a test data, list of sentences\n",
    "# Writes the output to a file, i.e. to test.out\n",
    "def predict_test2(model, sentences, fname):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sentence in sentences:\n",
    "            spelling_sentence = [get_spelling_feature(sentence)]\n",
    "            spelling_sentence = torch.from_numpy(np.array(spelling_sentence, dtype=np.int64)).to(device)\n",
    "            \n",
    "            transformed_sentence = [prepare_sequence(sentence, word_to_ix, use_unk=True)]\n",
    "            transformed_sentence = torch.from_numpy(np.array(transformed_sentence, dtype=np.int64)).to(device)\n",
    "            \n",
    "            y_pred_scores = model(transformed_sentence, spelling_sentence)\n",
    "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
    "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
    "\n",
    "            idx = 1\n",
    "            output = []\n",
    "            for i in range(len(y_pred_flat)):\n",
    "                word = sentence[i]\n",
    "                pred = ix_to_tag[y_pred_flat[i]]\n",
    "                if word == '<PAD>':\n",
    "                    break\n",
    "                output.append((idx, word, pred))\n",
    "                idx += 1\n",
    "            outputs.append(output)\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        for i in range(len(outputs)):\n",
    "            for j in range(len(outputs[i])):\n",
    "                idx, word, pred = outputs[i][j]\n",
    "                f.write(f'{idx} {word} {pred}\\n')\n",
    "            if i != len(outputs)-1:\n",
    "                f.write('\\n')\n",
    "                \n",
    "# Used to predict on a development data loader\n",
    "# Writes statistics to console\n",
    "def predict2(model, data_loader, message):\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y, X_original, X_spelling in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred_scores = model(X, X_spelling)\n",
    "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
    "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
    "            y_flat = torch.flatten(y).tolist()\n",
    "            \n",
    "            for i in range(len(y_pred_flat)):\n",
    "                if y_flat[i] == tag_to_ix['<PAD>']:\n",
    "                    break\n",
    "                all_y.append(y_flat[i])\n",
    "                all_y_pred.append(y_pred_flat[i])\n",
    "\n",
    "    print(message, classification_report(all_y, all_y_pred))\n",
    "                \n",
    "# Used to predict on a development data loader\n",
    "# Writes the output to a file for PERL script, i.e. to prediction.txt\n",
    "def predict_perl2(model, data_loader, fname):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y, X_original, X_spelling in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred_scores = model(X, X_spelling)\n",
    "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
    "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
    "            y_flat = torch.flatten(y).tolist()\n",
    "\n",
    "            idx = 1\n",
    "            output = []\n",
    "            for i in range(len(y_pred_flat)):\n",
    "                word = X_original[i][0]\n",
    "                gold = ix_to_tag[y_flat[i]]\n",
    "                pred = ix_to_tag[y_pred_flat[i]]\n",
    "                if word == '<PAD>':\n",
    "                    break\n",
    "                output.append((idx, word, gold, pred))\n",
    "                idx += 1\n",
    "            outputs.append(output)\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        for i in range(len(outputs)):\n",
    "            for j in range(len(outputs[i])):\n",
    "                idx, word, gold, pred = outputs[i][j]\n",
    "                f.write(f'{idx} {word} {gold} {pred}\\n')\n",
    "            if i != len(outputs)-1:\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dev_dataset = NERDataset(dev_data)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BLSTM2(VOCAB_SIZE, EMBEDDING_DIM, LSTM_HIDDEN_DIM, LINEAR_DIM, TAGS_SIZE, LSTM_DROPOUT, ELU_ALPHA,\n",
    "               embedding_matrix, SPELLING_EMBEDDING_DIM).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=SCHEDULER_STEP_SIZE, gamma=SCHEDULER_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLSTM2(\n",
       "  (embeddings_word): Embedding(30292, 100, padding_idx=25957)\n",
       "  (embeddings_spelling): Embedding(5, 20, padding_idx=0)\n",
       "  (dropout_pre_lstm): Dropout(p=0.33, inplace=False)\n",
       "  (lstm): LSTM(120, 256, batch_first=True, bidirectional=True)\n",
       "  (dropout_post_lstm): Dropout(p=0.33, inplace=False)\n",
       "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (elu): ELU(alpha=0.5)\n",
       "  (linear2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 blstm2.pt does not exist. Training a new model...\n",
      "Epoch 1 / 20, training loss: 0.01967, learning rate: 0.30000\n",
      "Epoch 1 / 20               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.87      0.86      0.87      1341\n",
      "           2       0.99      1.00      0.99     42975\n",
      "           3       0.94      0.78      0.85       922\n",
      "           4       0.91      0.97      0.94      1842\n",
      "           5       0.97      0.97      0.97      1307\n",
      "           6       0.92      0.94      0.93      1837\n",
      "           7       0.82      0.74      0.78       751\n",
      "           8       0.93      0.51      0.66       346\n",
      "           9       0.80      0.82      0.81       257\n",
      "\n",
      "    accuracy                           0.98     51578\n",
      "   macro avg       0.91      0.84      0.87     51578\n",
      "weighted avg       0.98      0.98      0.98     51578\n",
      "\n",
      "Epoch 2 / 20, training loss: 0.01013, learning rate: 0.30000\n",
      "Epoch 3 / 20, training loss: 0.00772, learning rate: 0.30000\n",
      "Epoch 4 / 20, training loss: 0.00624, learning rate: 0.30000\n",
      "Epoch 5 / 20, training loss: 0.00526, learning rate: 0.30000\n",
      "Epoch 5 / 20               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.89      0.91      1341\n",
      "           2       0.99      1.00      1.00     42975\n",
      "           3       0.93      0.84      0.88       922\n",
      "           4       0.95      0.98      0.96      1842\n",
      "           5       0.97      0.97      0.97      1307\n",
      "           6       0.96      0.95      0.95      1837\n",
      "           7       0.85      0.87      0.86       751\n",
      "           8       0.89      0.71      0.79       346\n",
      "           9       0.95      0.75      0.84       257\n",
      "\n",
      "    accuracy                           0.98     51578\n",
      "   macro avg       0.94      0.88      0.91     51578\n",
      "weighted avg       0.98      0.98      0.98     51578\n",
      "\n",
      "Epoch 6 / 20, training loss: 0.00375, learning rate: 0.15000\n",
      "Epoch 7 / 20, training loss: 0.00324, learning rate: 0.15000\n",
      "Epoch 8 / 20, training loss: 0.00300, learning rate: 0.15000\n",
      "Epoch 9 / 20, training loss: 0.00268, learning rate: 0.15000\n",
      "Epoch 10 / 20, training loss: 0.00244, learning rate: 0.15000\n",
      "Epoch 10 / 20               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.92      0.93      1341\n",
      "           2       1.00      1.00      1.00     42975\n",
      "           3       0.95      0.85      0.90       922\n",
      "           4       0.96      0.98      0.97      1842\n",
      "           5       0.98      0.97      0.98      1307\n",
      "           6       0.96      0.97      0.96      1837\n",
      "           7       0.93      0.87      0.90       751\n",
      "           8       0.83      0.82      0.83       346\n",
      "           9       0.90      0.93      0.92       257\n",
      "\n",
      "    accuracy                           0.99     51578\n",
      "   macro avg       0.94      0.92      0.93     51578\n",
      "weighted avg       0.99      0.99      0.99     51578\n",
      "\n",
      "Epoch 11 / 20, training loss: 0.00205, learning rate: 0.07500\n",
      "Epoch 12 / 20, training loss: 0.00190, learning rate: 0.07500\n",
      "Epoch 13 / 20, training loss: 0.00180, learning rate: 0.07500\n",
      "Epoch 14 / 20, training loss: 0.00169, learning rate: 0.07500\n",
      "Epoch 15 / 20, training loss: 0.00158, learning rate: 0.07500\n",
      "Epoch 15 / 20               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.93      0.93      1341\n",
      "           2       1.00      1.00      1.00     42975\n",
      "           3       0.93      0.89      0.91       922\n",
      "           4       0.96      0.98      0.97      1842\n",
      "           5       0.98      0.97      0.98      1307\n",
      "           6       0.96      0.97      0.96      1837\n",
      "           7       0.95      0.87      0.91       751\n",
      "           8       0.87      0.82      0.84       346\n",
      "           9       0.89      0.95      0.92       257\n",
      "\n",
      "    accuracy                           0.99     51578\n",
      "   macro avg       0.94      0.93      0.93     51578\n",
      "weighted avg       0.99      0.99      0.99     51578\n",
      "\n",
      "Epoch 16 / 20, training loss: 0.00155, learning rate: 0.03750\n",
      "Epoch 17 / 20, training loss: 0.00133, learning rate: 0.03750\n",
      "Epoch 18 / 20, training loss: 0.00129, learning rate: 0.03750\n",
      "Epoch 19 / 20, training loss: 0.00119, learning rate: 0.03750\n",
      "Epoch 20 / 20, training loss: 0.00130, learning rate: 0.03750\n",
      "Epoch 20 / 20               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.93      0.94      1341\n",
      "           2       1.00      1.00      1.00     42975\n",
      "           3       0.92      0.89      0.91       922\n",
      "           4       0.96      0.98      0.97      1842\n",
      "           5       0.98      0.97      0.98      1307\n",
      "           6       0.96      0.97      0.97      1837\n",
      "           7       0.93      0.87      0.90       751\n",
      "           8       0.86      0.80      0.83       346\n",
      "           9       0.89      0.95      0.92       257\n",
      "\n",
      "    accuracy                           0.99     51578\n",
      "   macro avg       0.94      0.93      0.93     51578\n",
      "weighted avg       0.99      0.99      0.99     51578\n",
      "\n",
      "Wall time: 28min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if os.path.isfile('blstm2.pt'):\n",
    "    print('Task 2', 'blstm2.pt exists. Loading existing model...')\n",
    "    model = torch.load('blstm2.pt')\n",
    "    model.to(device)\n",
    "else:\n",
    "    print('Task 2', 'blstm2.pt does not exist. Training a new model...')\n",
    "    total_loss = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        for i, (X, y, X_original, X_spelling) in enumerate(train_loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred_scores = model(X, X_spelling)\n",
    "            y_pred = torch.flatten(y_pred_scores, start_dim=0, end_dim=1)\n",
    "            y = torch.flatten(y)\n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss.append(loss.item())\n",
    "        print(f'Epoch {epoch+1} / {NUM_EPOCHS}, training loss: {np.average(total_loss):.5f}, learning rate: {optimizer.param_groups[0][\"lr\"]:.5f}')\n",
    "        total_loss = []\n",
    "        scheduler.step()\n",
    "        if epoch == 0 or (epoch+1) % 5 == 0:\n",
    "            predict2(model, dev_loader, f'Epoch {epoch+1} / {NUM_EPOCHS}')\n",
    "    torch.save(model, 'blstm2.pt')\n",
    "\n",
    "# Prediction for all cases (dev, test, and dev for perl)\n",
    "predict_perl2(model, dev_loader, 'prediction2.txt')\n",
    "predict_dev2(model, dev_loader, 'dev2.out')\n",
    "predict_test2(model, test_data, 'test2.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting structs for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_to_ix_2.pkl', 'wb') as f:\n",
    "    pickle.dump(word_to_ix, f)\n",
    "    \n",
    "with open('tag_to_ix_2.pkl', 'wb') as f:\n",
    "    pickle.dump(tag_to_ix, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
