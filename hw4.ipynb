{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install torch\n",
    "# !pip install sklearn\n",
    "# !pip install pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import os.path\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure reproducibility\n",
    "Use a fixed seed such that all steps and results can be reproduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 544\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Credit: From PyTorch's documentation\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fname, test_dataset=False):\n",
    "    sentences = []\n",
    "    with open(fname, 'r') as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "        sentence_words = []\n",
    "        sentence_tags = []\n",
    "        for line in lines:\n",
    "            if line:\n",
    "                # test data has only index and word\n",
    "                if test_dataset:\n",
    "                    index, word = line.split()\n",
    "                    sentence_words.append(word)\n",
    "                # train/dev data has index, word, and tag\n",
    "                else:\n",
    "                    index, word, tag = line.split()\n",
    "                    sentence_words.append(word)\n",
    "                    sentence_tags.append(tag)\n",
    "            else:\n",
    "                # Create a sentence upon reaching an empty new line\n",
    "                if test_dataset:\n",
    "                    sentences.append(sentence_words)\n",
    "                else:\n",
    "                    sentences.append((sentence_words, sentence_tags))\n",
    "                sentence_words = []\n",
    "                sentence_tags = []\n",
    "        # Create a sentence for the last sentence in the document\n",
    "        # incase it missed a newline in the document at the end\n",
    "        if len(sentence_words) > 0:\n",
    "            if test_dataset:\n",
    "                sentences.append(sentence_words)\n",
    "            else:\n",
    "                sentences.append((sentence_words, sentence_tags))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all datasets given\n",
    "dev_data = read_data('data/dev')\n",
    "test_data = read_data('data/test', test_dataset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions to process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix, use_unk=False):\n",
    "    if use_unk:\n",
    "        indices = [to_ix[w] if w in to_ix else to_ix['<UNK>'] for w in seq]\n",
    "    else:\n",
    "        indices = [to_ix[w] for w in seq]\n",
    "    return indices\n",
    "\n",
    "def get_spelling_feature(sentence):\n",
    "    result = []\n",
    "    for word in sentence:\n",
    "        # PAD = 0\n",
    "        if word == '<PAD>':\n",
    "            result.append(0)\n",
    "        ## ALL LOWER = 1\n",
    "        elif word.islower():\n",
    "            result.append(1)\n",
    "        # ALL UPPER = 2\n",
    "        elif word.isupper():\n",
    "            result.append(2)\n",
    "        # FIRST UPPER = 3\n",
    "        elif word[0].isupper():\n",
    "            result.append(3)\n",
    "        # OTHERS = 4\n",
    "        else:\n",
    "            result.append(4)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # Retrieves longest sentence, for padding\n",
    "        max_sentence_len = max([len(sentence) for sentence, tags in data])\n",
    "        self.X = []\n",
    "        self.X_original = []\n",
    "        self.y = []\n",
    "        self.X_spelling = []\n",
    "        \n",
    "        for sentence, tags in data:\n",
    "            # Pad the sentences to the same length\n",
    "            padded_sentence = sentence.copy()\n",
    "            padded_tags = tags.copy()\n",
    "            while len(padded_sentence) < max_sentence_len:\n",
    "                padded_sentence.append('<PAD>')\n",
    "                padded_tags.append('<PAD>')\n",
    "            # Convert to indices\n",
    "            transformed_sentence = prepare_sequence(padded_sentence, word_to_ix, use_unk=True)\n",
    "            transformed_tags = prepare_sequence(padded_tags, tag_to_ix)\n",
    "            # Get spelling indices\n",
    "            spelling_sentence = get_spelling_feature(padded_sentence)\n",
    "            # Add to dataset\n",
    "            self.X.append(transformed_sentence)\n",
    "            self.X_original.append(padded_sentence)\n",
    "            self.y.append(transformed_tags)\n",
    "            self.X_spelling.append(spelling_sentence)\n",
    "            \n",
    "        self.X = torch.from_numpy(np.array(self.X, dtype=np.int64)).to(device)\n",
    "        self.y = torch.from_numpy(np.array(self.y, dtype=np.int64)).to(device)\n",
    "        self.X_spelling = torch.from_numpy(np.array(self.X_spelling, dtype=np.int64)).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index], self.X_original[index], self.X_spelling[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Simple Bidirectional LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load vocab and word/tag -> index, and index -> word/tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_to_ix_1.pkl', 'rb') as f:\n",
    "    word_to_ix = pickle.load(f)\n",
    "    \n",
    "with open('tag_to_ix_1.pkl', 'rb') as f:\n",
    "    tag_to_ix = pickle.load(f)\n",
    "           \n",
    "# Generate index to word/tag mappings\n",
    "ix_to_word = {v: k for k, v in word_to_ix.items()}\n",
    "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
    "\n",
    "# Calculate the size of vocabulary & tags\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "TAGS_SIZE = len(tag_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM Model with random embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM1(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, linear_dim, tags_size, lstm_dropout, elu_alpha):\n",
    "        super(BLSTM1, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=word_to_ix['<PAD>'])\n",
    "        self.dropout_pre_lstm = nn.Dropout(lstm_dropout)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout_post_lstm = nn.Dropout(lstm_dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, linear_dim)\n",
    "        self.elu = nn.ELU(alpha=elu_alpha)\n",
    "        self.linear2 = nn.Linear(linear_dim, tags_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout_pre_lstm(x)\n",
    "        \n",
    "        h0 = torch.zeros(2, x.size(0), self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(2, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        out = self.dropout_post_lstm(out)\n",
    "        out = self.linear(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.linear2(out)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to predict on a development data loader\n",
    "# Writes the output to a file, i.e. to dev.out\n",
    "def predict_dev1(model, data_loader, fname):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y, X_original, X_spelling in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred_scores = model(X)\n",
    "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
    "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
    "\n",
    "            idx = 1\n",
    "            output = []\n",
    "            for i in range(len(y_pred_flat)):\n",
    "                word = X_original[i][0]\n",
    "                pred = ix_to_tag[y_pred_flat[i]]\n",
    "                if word == '<PAD>':\n",
    "                    break\n",
    "                output.append((idx, word, pred))\n",
    "                idx += 1\n",
    "            outputs.append(output)\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        for i in range(len(outputs)):\n",
    "            for j in range(len(outputs[i])):\n",
    "                idx, word, pred = outputs[i][j]\n",
    "                f.write(f'{idx} {word} {pred}\\n')\n",
    "            if i != len(outputs)-1:\n",
    "                f.write('\\n')\n",
    "\n",
    "# Used to predict on a test data, list of sentences\n",
    "# Writes the output to a file, i.e. to test.out\n",
    "def predict_test1(model, sentences, fname):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sentence in sentences:\n",
    "            spelling_sentence = [get_spelling_feature(sentence)]\n",
    "            spelling_sentence = torch.from_numpy(np.array(spelling_sentence, dtype=np.int64)).to(device)\n",
    "            \n",
    "            transformed_sentence = [prepare_sequence(sentence, word_to_ix, use_unk=True)]\n",
    "            transformed_sentence = torch.from_numpy(np.array(transformed_sentence, dtype=np.int64)).to(device)\n",
    "            \n",
    "            y_pred_scores = model(transformed_sentence)\n",
    "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
    "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
    "\n",
    "            idx = 1\n",
    "            output = []\n",
    "            for i in range(len(y_pred_flat)):\n",
    "                word = sentence[i]\n",
    "                pred = ix_to_tag[y_pred_flat[i]]\n",
    "                if word == '<PAD>':\n",
    "                    break\n",
    "                output.append((idx, word, pred))\n",
    "                idx += 1\n",
    "            outputs.append(output)\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        for i in range(len(outputs)):\n",
    "            for j in range(len(outputs[i])):\n",
    "                idx, word, pred = outputs[i][j]\n",
    "                f.write(f'{idx} {word} {pred}\\n')\n",
    "            if i != len(outputs)-1:\n",
    "                f.write('\\n')\n",
    "                \n",
    "# Used to predict on a development data loader\n",
    "# Writes the output to a file for PERL script, i.e. to prediction.txt\n",
    "def predict_perl1(model, data_loader, fname):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y, X_original, X_spelling in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred_scores = model(X)\n",
    "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
    "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
    "            y_flat = torch.flatten(y).tolist()\n",
    "\n",
    "            idx = 1\n",
    "            output = []\n",
    "            for i in range(len(y_pred_flat)):\n",
    "                word = X_original[i][0]\n",
    "                gold = ix_to_tag[y_flat[i]]\n",
    "                pred = ix_to_tag[y_pred_flat[i]]\n",
    "                if word == '<PAD>':\n",
    "                    break\n",
    "                output.append((idx, word, gold, pred))\n",
    "                idx += 1\n",
    "            outputs.append(output)\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        for i in range(len(outputs)):\n",
    "            for j in range(len(outputs[i])):\n",
    "                idx, word, gold, pred = outputs[i][j]\n",
    "                f.write(f'{idx} {word} {gold} {pred}\\n')\n",
    "            if i != len(outputs)-1:\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert dev data to data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = NERDataset(dev_data)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLSTM1(\n",
       "  (embeddings): Embedding(8129, 100, padding_idx=0)\n",
       "  (dropout_pre_lstm): Dropout(p=0.33, inplace=False)\n",
       "  (lstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
       "  (dropout_post_lstm): Dropout(p=0.33, inplace=False)\n",
       "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (elu): ELU(alpha=0.5)\n",
       "  (linear2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = torch.load('blstm1.pt')\n",
    "model_1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLSTM1(\n",
       "  (embeddings): Embedding(8129, 100, padding_idx=0)\n",
       "  (dropout_pre_lstm): Dropout(p=0.33, inplace=False)\n",
       "  (lstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
       "  (dropout_post_lstm): Dropout(p=0.33, inplace=False)\n",
       "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (elu): ELU(alpha=0.5)\n",
       "  (linear2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Prediction for all cases (dev, test, and dev for perl)\n",
    "predict_perl1(model_1, dev_loader, 'prediction1.txt')\n",
    "predict_dev1(model_1, dev_loader, 'dev1.out')\n",
    "predict_test1(model_1, test_data, 'test1.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Using GloVe word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load vocab and word/tag -> index, and index -> word/tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_to_ix_2.pkl', 'rb') as f:\n",
    "    word_to_ix = pickle.load(f)\n",
    "    \n",
    "with open('tag_to_ix_2.pkl', 'rb') as f:\n",
    "    tag_to_ix = pickle.load(f)\n",
    "           \n",
    "# Generate index to word/tag mappings\n",
    "ix_to_word = {v: k for k, v in word_to_ix.items()}\n",
    "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
    "\n",
    "# Calculate the size of vocabulary & tags\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "TAGS_SIZE = len(tag_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM Model with GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, linear_dim, tags_size, lstm_dropout, elu_alpha, embeddings, spelling_embedding_dim):\n",
    "        super(BLSTM2, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embeddings_word = nn.Embedding.from_pretrained(torch.from_numpy(embeddings).float(), freeze=False, padding_idx=word_to_ix['<PAD>'])\n",
    "        self.embeddings_spelling = nn.Embedding(num_embeddings=5, embedding_dim=spelling_embedding_dim, padding_idx=0)\n",
    "        self.dropout_pre_lstm = nn.Dropout(lstm_dropout)\n",
    "        self.lstm = nn.LSTM(embedding_dim+spelling_embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout_post_lstm = nn.Dropout(lstm_dropout)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, linear_dim)\n",
    "        self.elu = nn.ELU(alpha=elu_alpha)\n",
    "        self.linear2 = nn.Linear(linear_dim, tags_size)\n",
    "    \n",
    "    def forward(self, x_word, x_spelling):\n",
    "        x1 = self.embeddings_word(x_word)\n",
    "        x2 = self.embeddings_spelling(x_spelling)\n",
    "        x = torch.cat((x1, x2), dim=2).to(device)\n",
    "        x = self.dropout_pre_lstm(x)\n",
    "        \n",
    "        h0 = torch.zeros(2, x.size(0), self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(2, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        out = self.dropout_post_lstm(out)\n",
    "        out = self.linear(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.linear2(out)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to predict on a development data loader\n",
    "# Writes the output to a file, i.e. to dev.out\n",
    "def predict_dev2(model, data_loader, fname):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y, X_original, X_spelling in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred_scores = model(X, X_spelling)\n",
    "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
    "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
    "\n",
    "            idx = 1\n",
    "            output = []\n",
    "            for i in range(len(y_pred_flat)):\n",
    "                word = X_original[i][0]\n",
    "                pred = ix_to_tag[y_pred_flat[i]]\n",
    "                if word == '<PAD>':\n",
    "                    break\n",
    "                output.append((idx, word, pred))\n",
    "                idx += 1\n",
    "            outputs.append(output)\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        for i in range(len(outputs)):\n",
    "            for j in range(len(outputs[i])):\n",
    "                idx, word, pred = outputs[i][j]\n",
    "                f.write(f'{idx} {word} {pred}\\n')\n",
    "            if i != len(outputs)-1:\n",
    "                f.write('\\n')\n",
    "\n",
    "# Used to predict on a test data, list of sentences\n",
    "# Writes the output to a file, i.e. to test.out\n",
    "def predict_test2(model, sentences, fname):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sentence in sentences:\n",
    "            spelling_sentence = [get_spelling_feature(sentence)]\n",
    "            spelling_sentence = torch.from_numpy(np.array(spelling_sentence, dtype=np.int64)).to(device)\n",
    "            \n",
    "            transformed_sentence = [prepare_sequence(sentence, word_to_ix, use_unk=True)]\n",
    "            transformed_sentence = torch.from_numpy(np.array(transformed_sentence, dtype=np.int64)).to(device)\n",
    "            \n",
    "            y_pred_scores = model(transformed_sentence, spelling_sentence)\n",
    "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
    "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
    "\n",
    "            idx = 1\n",
    "            output = []\n",
    "            for i in range(len(y_pred_flat)):\n",
    "                word = sentence[i]\n",
    "                pred = ix_to_tag[y_pred_flat[i]]\n",
    "                if word == '<PAD>':\n",
    "                    break\n",
    "                output.append((idx, word, pred))\n",
    "                idx += 1\n",
    "            outputs.append(output)\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        for i in range(len(outputs)):\n",
    "            for j in range(len(outputs[i])):\n",
    "                idx, word, pred = outputs[i][j]\n",
    "                f.write(f'{idx} {word} {pred}\\n')\n",
    "            if i != len(outputs)-1:\n",
    "                f.write('\\n')\n",
    "                \n",
    "# Used to predict on a development data loader\n",
    "# Writes the output to a file for PERL script, i.e. to prediction.txt\n",
    "def predict_perl2(model, data_loader, fname):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y, X_original, X_spelling in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred_scores = model(X, X_spelling)\n",
    "            y_pred = torch.argmax(y_pred_scores, dim=2)\n",
    "            y_pred_flat = torch.flatten(y_pred).tolist()\n",
    "            y_flat = torch.flatten(y).tolist()\n",
    "\n",
    "            idx = 1\n",
    "            output = []\n",
    "            for i in range(len(y_pred_flat)):\n",
    "                word = X_original[i][0]\n",
    "                gold = ix_to_tag[y_flat[i]]\n",
    "                pred = ix_to_tag[y_pred_flat[i]]\n",
    "                if word == '<PAD>':\n",
    "                    break\n",
    "                output.append((idx, word, gold, pred))\n",
    "                idx += 1\n",
    "            outputs.append(output)\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        for i in range(len(outputs)):\n",
    "            for j in range(len(outputs[i])):\n",
    "                idx, word, gold, pred = outputs[i][j]\n",
    "                f.write(f'{idx} {word} {gold} {pred}\\n')\n",
    "            if i != len(outputs)-1:\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert dev data to data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = NERDataset(dev_data)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLSTM2(\n",
       "  (embeddings_word): Embedding(30292, 100, padding_idx=25957)\n",
       "  (embeddings_spelling): Embedding(5, 20, padding_idx=0)\n",
       "  (dropout_pre_lstm): Dropout(p=0.33, inplace=False)\n",
       "  (lstm): LSTM(120, 256, batch_first=True, bidirectional=True)\n",
       "  (dropout_post_lstm): Dropout(p=0.33, inplace=False)\n",
       "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (elu): ELU(alpha=0.5)\n",
       "  (linear2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = torch.load('blstm2.pt')\n",
    "model_2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLSTM2(\n",
       "  (embeddings_word): Embedding(30292, 100, padding_idx=25957)\n",
       "  (embeddings_spelling): Embedding(5, 20, padding_idx=0)\n",
       "  (dropout_pre_lstm): Dropout(p=0.33, inplace=False)\n",
       "  (lstm): LSTM(120, 256, batch_first=True, bidirectional=True)\n",
       "  (dropout_post_lstm): Dropout(p=0.33, inplace=False)\n",
       "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (elu): ELU(alpha=0.5)\n",
       "  (linear2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Prediction for all cases (dev, test, and dev for perl)\n",
    "predict_perl2(model_2, dev_loader, 'prediction2.txt')\n",
    "predict_dev2(model_2, dev_loader, 'dev2.out')\n",
    "predict_test2(model_2, test_data, 'test2.out')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
